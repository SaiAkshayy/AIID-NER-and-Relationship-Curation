{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21da5104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\saiak\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\saiak\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\saiak\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.25.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.20.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\saiak\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb8fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a65748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering completed. Results saved to 'Clustered_Entities.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the original dataset\n",
    "data = pd.read_csv(\"AI_Incident_Dataset.csv\")\n",
    "\n",
    "# Combine entities from all columns and ensure uniqueness\n",
    "unique_entities = pd.concat([data['Deployer'], data['Developer'], data['Victim']]).drop_duplicates().tolist()\n",
    "\n",
    "# Initialize the sentence embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the unique entities\n",
    "embeddings = embedding_model.encode(unique_entities)\n",
    "\n",
    "# Apply K-Means clustering with 100 clusters\n",
    "num_clusters = 100\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Create a DataFrame to map unique entities to their clusters\n",
    "clustered_entities = pd.DataFrame({\n",
    "    'Entity': unique_entities,\n",
    "    'Cluster': cluster_labels\n",
    "})\n",
    "\n",
    "# Save the clustered unique entities to a CSV\n",
    "clustered_entities.to_csv(\"Clustered_Entities.csv\", index=False)\n",
    "\n",
    "print(\"Clustering completed. Results saved to 'Clustered_Entities.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edabc0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>judiciary of italy</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>meta</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york city government</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eric adams administration</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brookdale senior living</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Entity  Cluster\n",
       "0          judiciary of italy       17\n",
       "1                        meta        3\n",
       "2    new york city government       26\n",
       "3   eric adams administration       60\n",
       "4     brookdale senior living       67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc393f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Cluster' column and combine entities in each cluster\n",
    "clustered_rows = clustered_entities.groupby('Cluster')['Entity'].apply(list).reset_index()\n",
    "clustered_rows.to_csv('Stacked_Clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab8c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters stacked in Developer, Deployer, Victim columns saved to 'Clustered_By_Columns.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the clustered entities CSV\n",
    "clustered_data = pd.read_csv(\"Clustered_Entities.csv\")\n",
    "\n",
    "# Load the original dataset\n",
    "original_data = pd.read_csv(\"AI_Incident_Dataset.csv\")\n",
    "\n",
    "# Melt the original dataset to create a long format\n",
    "melted_data = original_data.melt(var_name=\"Column\", value_name=\"Entity\")\n",
    "\n",
    "# Deduplicate melted data by entity\n",
    "melted_data = melted_data.drop_duplicates(subset=[\"Entity\", \"Column\"])\n",
    "\n",
    "# Merge cluster assignments back with melted data\n",
    "clustered_data = pd.merge(melted_data, clustered_data, on=\"Entity\", how=\"inner\")\n",
    "\n",
    "# Group by Cluster and Column, deduplicate entities, and reset index\n",
    "clustered_grouped = (\n",
    "    clustered_data\n",
    "    .groupby(['Cluster', 'Column'])['Entity']\n",
    "    .apply(lambda x: list(set(x)))  # Deduplicate entities within each group\n",
    "    .unstack(fill_value=[])         # Unstack columns for Developer, Deployer, Victim\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Save to a new CSV for analysis\n",
    "clustered_grouped.to_csv(\"Clustered_By_Columns.csv\", index=False)\n",
    "\n",
    "print(\"Clusters stacked in Developer, Deployer, Victim columns saved to 'Clustered_By_Columns.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f36353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78d71024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the grouped data with cluster labels\n",
    "grouped_clusters = pd.read_csv(\"Clustered_By_Columns_labels.csv\")\n",
    "\n",
    "# Melt the data back to a long format to associate roles with clusters\n",
    "long_format = grouped_clusters.melt(\n",
    "    id_vars=[\"Cluster\", \"Cluster_label\"], \n",
    "    value_vars=[\"Developer\", \"Deployer\", \"Victim\"], \n",
    "    var_name=\"Role\", \n",
    "    value_name=\"Entities\"\n",
    ")\n",
    "\n",
    "# Explode the entities list to create one row per entity\n",
    "long_format = long_format.explode(\"Entities\").dropna(subset=[\"Entities\"])\n",
    "\n",
    "# Save the long format with roles and clusters\n",
    "long_format.to_csv(\"Mapped_Entities_With_Roles.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963f725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
